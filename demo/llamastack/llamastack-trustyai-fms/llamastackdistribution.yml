apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: llamastack-trustyai-fms
  namespace: summit-connect-2025
spec:
  replicas: 1
  server:
    containerSpec:
      env:
        - name: VLLM_URL
          # Default to tinyllama-1b (faster deployment, requires 1 GPU)
          # Alternative: llama32-1b (better quality, requires 1 GPU)
          # Uses OpenShift Route with HTTPS for external access
          # Get route: oc get route tinyllama-1b-predictor -n summit-connect-2025 -o jsonpath='{.spec.host}'
          value: 'https://tinyllama-1b-predictor-summit-connect-2025.apps.CLUSTER_DOMAIN/v1'
        - name: INFERENCE_MODEL
          value: 'tinyllama-1b'
        - name: MILVUS_DB_PATH
          value: '~/.llama/milvus.db'
        - name: VLLM_TLS_VERIFY
          value: 'false'
        - name: FMS_ORCHESTRATOR_URL
          value: 'http://guardrails-orchestrator-service:8033'
      name: llama-stack
      port: 8321
    distribution:
      name: rh-dev
    storage:
      size: 20Gi